{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PBZg-tg1Bm0I"
   },
   "source": [
    "# Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ET6cJ7V0Bm0N"
   },
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J3IeRwHUBm0R"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from random import shuffle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y1MNsiI1Bm0e"
   },
   "outputs": [],
   "source": [
    "# Comment to get non-deterministic results\n",
    "np.random.seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "HE8sXqyQCSnl",
    "outputId": "8a15547d-038f-4767-c546-70c7916881aa"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "#drive.mount('/content/gdrive', force_remount=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e831pmeVBm0r"
   },
   "source": [
    "## Problem 1.\n",
    "\n",
    "**Building the Model**\n",
    "\n",
    "1.    Forward and backward propagation for single exemples (forward and backward functions)\n",
    "    and for matrices (forward_mbatch and backward_mbatch functions)\n",
    "    \n",
    "2.    This implemetation does not use any deep learning frameworks with automatic differentiation\n",
    "\n",
    "3. We trained the model using the probability loss (cross entropy) as training criterion. See the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_1hAuBgWBm0u"
   },
   "outputs": [],
   "source": [
    "#Several functions in this class implementation are inspired from the NN implemented in cours IFT6093\n",
    "\n",
    "class NN(object):\n",
    "    \n",
    "    '''\n",
    "    Arguments:\n",
    "        input_dim: The input dimension\n",
    "        output_dim: The output dimension\n",
    "        hidden_dims: (h1 dimension, h2 dimension)\n",
    "        n_hidden: number of hidden layers\n",
    "        initialization: type of weigth initialization (zeros, normal or glorot)\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim,hidden_dims=(1024,2048),n_hidden=2, initialization='zeros', mode=',train',\n",
    "                 datapath=None,model_path=None):\n",
    "        \n",
    "        self.indim = input_dim\n",
    "        self.hd1 = hidden_dims[0] \n",
    "        self.hd2 = hidden_dims[1]\n",
    "        self.n_hidden = n_hidden\n",
    "        self.outd = output_dim\n",
    "        self.W1 = np.zeros(shape=(hidden_dims[0], input_dim))\n",
    "        self.b1 = np.zeros(hidden_dims[0])\n",
    "        self.W2 = np.zeros(shape=(hidden_dims[1], hidden_dims[0]))\n",
    "        self.b2 = np.zeros(hidden_dims[1])\n",
    "        self.W3 = np.zeros(shape=(output_dim, hidden_dims[1]))\n",
    "        self.b3 = np.zeros(output_dim)\n",
    "        \n",
    "        \n",
    "        if initialization=='normal':\n",
    "            self.initialize_weights_normal()\n",
    "            \n",
    "        if initialization=='glorot':\n",
    "            self.initialize_weights_glorot()\n",
    "           \n",
    "        self.parameters = [self.W3, self.b3, self.W2, self.b2, self.W1, self.b1]\n",
    "        \n",
    "        \n",
    "    def initialize_weights_normal(self):\n",
    "        \n",
    "        self.W1 = np.random.normal(size=(self.hd1, self.indim))\n",
    "        self.W2 = np.random.normal(size=(self.hd2, self.hd1))\n",
    "        self.W3 = np.random.normal(size=(self.outd, self.hd2))\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    \n",
    "    def initialize_weights_glorot(self):\n",
    "        \n",
    "        dl1 = np.sqrt(6/(self.indim + self.hd1))\n",
    "        dl2 = np.sqrt(6/(self.hd1 + self.hd2))\n",
    "        dl3 = np.sqrt(6/(self.hd2 + self.outd))\n",
    "        self.W1 = np.random.uniform(low=(-dl1), high=dl1, size=(self.hd1, self.indim))\n",
    "        self.W2 = np.random.uniform(low=(-dl2), high=dl2, size=(self.hd2, self.hd1))\n",
    "        self.W3 = np.random.uniform(low=(-dl3), high=dl3, size=(self.outd, self.hd2))\n",
    "        return self\n",
    "        \n",
    "        \n",
    "    #Method from NN implemented in cours IFT6093\n",
    "    def activation (self,input):\n",
    "        \n",
    "        return (input > 0) * input  \n",
    "    \n",
    "    #line 85\n",
    "\n",
    "    \n",
    "    # forward for single exemple\n",
    "    def forward(self,x):\n",
    "                \n",
    "        a1 = np.dot (self.W1, x) + self.b1 \n",
    "        h1 = self.activation (a1)\n",
    "        a2 = np.dot (self.W2, h1) + self.b2\n",
    "        h2 = self.activation (a2)\n",
    "        oa = np.dot (self.W3, h2) + self.b3\n",
    "        os = self.softmax (oa, axis=0)\n",
    "        \n",
    "        return a1, h1, a2, h2, oa, os\n",
    "    \n",
    "    def setZero(self, a ):\n",
    "        a[ np.abs( a ) <= zero ] = zero\n",
    "        return a\n",
    "\n",
    "    #Loss function using probability loss (cross entropie)\n",
    "    def loss (self, y, os):\n",
    "        return (self.setZero(y) * (-np.log(self.setZero(os)))).sum()\n",
    "    \n",
    "    #ans softmax methods from NN implemented in cours IFT6093\n",
    "    def softmax (self,x,axis=1):\n",
    "        shiftx = x - np.max (x, axis=axis, keepdims=True)\n",
    "        exps = np.exp (shiftx)\n",
    "        y = exps / exps.sum (axis=axis, keepdims=True)\n",
    "        return y\n",
    "\n",
    "\n",
    "    #Backward for single exemple\n",
    "    def backward(self, x, y, a1, h1, a2, h2, oa, os, weight_decay=0, cache=None):\n",
    "        \n",
    "        grad_oa = os - y\n",
    "        grad_W3 = np.outer (grad_oa, h2) + weight_decay * self.W3\n",
    "        grad_b3 = grad_oa\n",
    "        grad_h2 = np.dot (self.W3.T, grad_oa)\n",
    "        grad_a2 = (a2 > 0) * grad_h2\n",
    "        grad_W2 = np.outer (grad_a2, h1) + weight_decay * self.W2\n",
    "        grad_b2 = grad_a2\n",
    "        grad_h1 = np.dot (self.W2.T, grad_a2)\n",
    "        grad_a1 = (a1 > 0) * grad_h1\n",
    "        grad_W1 = np.outer (grad_a1, x) + weight_decay * self.W1\n",
    "        grad_b1 = grad_a1\n",
    "        grads=[grad_W3, grad_b3, grad_W2, grad_b2, grad_W1, grad_b1]\n",
    "   \n",
    "        return grads\n",
    "\n",
    "\n",
    "\n",
    "    def update(self, grads, learning_rate):\n",
    "        for p, grad in zip(self.parameters, grads):\n",
    "            p -= learning_rate * grad\n",
    "        \n",
    "    #line 201   \n",
    "\n",
    "    #Sthocastic gradient descent method (litteral)\n",
    "    def train_SGD(self, x, y_onehot, n, learning_rate=1e-1, weight_decay=0):\n",
    "                \n",
    "        y = y_onehot\n",
    "        losses = 0\n",
    "        if (n==1):\n",
    "            a1, h1, a2, h2, oa, os = self.forward(x)\n",
    "            grads = self.backward(x, y, a1, h1, a2, h2, oa, os)\n",
    "            self.update(grads, learning_rate)\n",
    "            loss = self.loss(y, os)\n",
    "            losses += loss  \n",
    "            average_loss = losses / x.shape[0]\n",
    "        else:    \n",
    "            for j in range(x.shape[0]):\n",
    "                a1, h1, a2, h2, oa, os = self.forward(x[j])\n",
    "                grads = self.backward(x[j], y[j], a1, h1, a2, h2, oa, os)\n",
    "                self.update(grads, learning_rate)\n",
    "                loss = self.loss(y[j], os)\n",
    "                losses += loss     \n",
    "            average_loss = losses / x.shape[0]\n",
    "                \n",
    "        return average_loss\n",
    "    \n",
    "    \n",
    "    def prediction_SGD (self, x):\n",
    "        predictions = np.zeros(x.shape[0])\n",
    "        for i in range(x.shape[0]):\n",
    "            _, _, _, _, _, os = self.forward(x[i])\n",
    "            predictions[i] = os.argmax()\n",
    "                \n",
    "        return predictions\n",
    "                \n",
    "    \n",
    "    def accuracy_SGD (self, prediction, y):\n",
    "        accuracies=0\n",
    "        for i in range (y.shape[0]):\n",
    "            accuracies+=(prediction[i]==y[i])\n",
    "            \n",
    "        return accuracies / y.shape[0]\n",
    "    \n",
    "    \n",
    "    def test_SGD(self, x, y_onehot, y):\n",
    "        pred=np.zeros(y.shape[0])\n",
    "        avg_loss=0\n",
    "        for i in range (x.shape[0]):\n",
    "            _, _, _, _, _, os = self.forward(x[i])\n",
    "            loss=self.loss (y_onehot[i], os)\n",
    "            avg_loss+=loss\n",
    "            pred[i]=os.argmax()\n",
    "            \n",
    "        accuracy=self.accuracy_SGD(pred, y)    \n",
    "        return avg_loss / x.shape[0] , accuracy\n",
    "    \n",
    "   \n",
    "    #Sthocastic Gradien Descent method with minibatch (using matrices)\n",
    "    def forward_mbatch(self, x):\n",
    "                \n",
    "        a1 = np.dot ( x, self.W1.T) + self.b1\n",
    "        h1 = self.activation (a1)\n",
    "        a2 = np.dot (h1, self.W2.T) + self.b2\n",
    "        h2 = self.activation (a2)\n",
    "        oa = np.dot (h2, self.W3.T) + self.b3\n",
    "        os = self.softmax (oa, axis=1)\n",
    "                \n",
    "        return a1, h1, a2, h2, oa, os\n",
    "    \n",
    "    #line 303\n",
    "        \n",
    "    def backward_mbatch(self, x, y, a1, h1, a2, h2, oa, os, batch_n, weight_decay=0):\n",
    "                \n",
    "        batch_n = x.shape[0]\n",
    "        bgrad_oa = os - y\n",
    "        bgrad_W3 = np.dot (bgrad_oa.T, h2) / batch_n  + weight_decay * self.W3\n",
    "        bgrad_b3 = bgrad_oa.mean(axis=0)\n",
    "        bgrad_h2 = np.dot (bgrad_oa, self.W3)\n",
    "        bgrad_a2 = (a2 > 0) * bgrad_h2\n",
    "        bgrad_W2 = np.dot (bgrad_a2.T, h1) / batch_n  + weight_decay * self.W2\n",
    "        bgrad_b2 = bgrad_a2.mean(axis=0)\n",
    "        bgrad_h1 = np.dot (bgrad_a2, self.W2)\n",
    "        bgrad_a1 = (a1 > 0) * bgrad_h1\n",
    "        bgrad_W1 = np.dot (bgrad_a1.T, x) / batch_n  + weight_decay * self.W1\n",
    "        bgrad_b1 = bgrad_a1.mean(axis=0)\n",
    "        bgrads=[bgrad_W3, bgrad_b3, bgrad_W2, bgrad_b2, bgrad_W1, bgrad_b1]\n",
    "   \n",
    "        return bgrads\n",
    "\n",
    "    #line 360\n",
    "\n",
    "    #Method taken fron homwork 3 in cours IFT6093\n",
    "    def loss_mbatch(self, os, y):\n",
    "        return (y * (-np.log(os))).sum(axis=1).mean(axis=0)     \n",
    "        \n",
    "    \n",
    "    #training with minibatch gradient decent\n",
    "    def train_mbatch(self, x, y_onehot, mb_size=100, learning_rate=1e-1, weight_decay=0):\n",
    "                \n",
    "        average_loss=0\n",
    "        for i in range (0, x.shape[0], mb_size):\n",
    "            xi = x[i:(i+mb_size)]\n",
    "            yi = y_onehot[i:(i+mb_size)]\n",
    "        \n",
    "            #losses = 0\n",
    "            a1, h1, a2, h2, oa, os = self.forward_mbatch(xi)\n",
    "            grads = self.backward_mbatch (xi, yi,a1, h1, a2, h2,oa, os, mb_size)\n",
    "            self.update(grads, learning_rate)\n",
    "            average_loss = self.loss_mbatch(os, yi) \n",
    "                          \n",
    "        return average_loss\n",
    "    \n",
    "    \n",
    "    #line 385\n",
    "    \n",
    "    def prediction_mbatch (self, x):\n",
    "        _, _, _, _, _, os = self.forward_mbatch(x)\n",
    "        return os.argmax(axis=1)\n",
    "    \n",
    "\n",
    "    def accuracy_mbatch (self, prediction, y):\n",
    "        accuracy = np.zeros(y.shape[0])\n",
    "        accuracy = prediction == y\n",
    "        return accuracy.mean(axis=0)\n",
    "    \n",
    "\n",
    "    def test_mbatch(self, x, y_onehot, y):\n",
    "        _, _, _, _, _, os = self.forward_mbatch(x)\n",
    "        loss = self.loss_mbatch (os, y_onehot)\n",
    "        accuracy=self.accuracy_mbatch (os.argmax(axis=1), y)\n",
    "        return loss, accuracy\n",
    "    \n",
    "    \n",
    "    def finite_difference():\n",
    "        \n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_twY2VYjBm00"
   },
   "source": [
    "## Function onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mMqPZPYiBm02"
   },
   "outputs": [],
   "source": [
    "#function taken from IFT6093 cours\n",
    "def onehot(y, n_classes):\n",
    "    o = np.zeros(shape=(y.shape[0], n_classes))\n",
    "    for i in range(y.shape[0]):\n",
    "        o[i, int(y[i])] = 1\n",
    "    return o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uO-BM7e3Bm0-"
   },
   "source": [
    "## Data loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3-Xusyx8Bm1Q"
   },
   "source": [
    "Getting data from the mnist.pkl.gz file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pnIZV_nZBuku"
   },
   "outputs": [],
   "source": [
    "import gzip, pickle\n",
    "with gzip.open('data/mnist.pkl.gz','rb') as ff :\n",
    "    u = pickle._Unpickler( ff )\n",
    "    u.encoding = 'latin1'\n",
    "    train, val, test = u.load()\n",
    "    \n",
    "X_train, y_train = train\n",
    "X_valid, y_valid = val\n",
    "X_test, y_test = test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1bIrarf-Bm1e"
   },
   "source": [
    "Processing data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "id": "tcBgwXwJBm1f",
    "outputId": "2379dc56-f86e-49cd-a1d3-ceee2ef2f250"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape =  (50000, 784)\n",
      "y_train shape =  (50000,)\n",
      "X_valid shape =  (10000, 784)\n",
      "y_valid shape =  (10000,)\n",
      "X_test shape =  (10000, 784)\n",
      "y_test shape =  (10000,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Data normalization\n",
    "\n",
    "X_train= X_train / 255\n",
    "X_valid= X_valid/ 255\n",
    "X_test = X_test / 255\n",
    "\n",
    "# train_data randomization\n",
    "#indices_t = list(range(len(X_train)))\n",
    "#shuffle(indices_t)\n",
    "#X_train, y_train=  X_train[indices_t[:]], y_train[indices_t[:]]\n",
    "\n",
    "# validation_data randomization\n",
    "#indices_v = list(range(len(X_valid)))\n",
    "#shuffle(indices_v)\n",
    "#X_valid, y_valid = X_train[indices_v[:]], y_train[indices_v[:]]\n",
    "\n",
    "#geting onehot(y)\n",
    "digit_y_train_onehot= onehot (y_train, 10)\n",
    "digit_y_valid_onehot= onehot (y_valid, 10)\n",
    "digit_y_test_onehot= onehot (y_test, 10)\n",
    "\n",
    "X_train = np.array (X_train)\n",
    "y_train = np.array (y_train)\n",
    "X_valid= np.array (X_valid)\n",
    "y_valid= np.array (y_valid)\n",
    "X_test = np.array (X_test)\n",
    "y_test = np.array (y_test)\n",
    "\n",
    "\n",
    "print('X_train shape = ', X_train.shape)\n",
    "print('y_train shape = ', y_train.shape)\n",
    "print('X_valid shape = ', X_valid.shape)\n",
    "print('y_valid shape = ', y_valid.shape)\n",
    "print('X_test shape = ', X_test.shape)\n",
    "print('y_test shape = ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zf8oqL3PIkv-"
   },
   "source": [
    "Learning curves function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gAmZ_6y1Bm1q"
   },
   "outputs": [],
   "source": [
    "#This function returns the learning curves grafics\n",
    "\n",
    "def learning_curves(losses_train, accs_train, losses_valid, accs_valid,\n",
    "                    losses_test, accs_test, h1, h2):\n",
    "  \n",
    " \n",
    "  plt.figure(figsize=(12, 4))\n",
    "  axis = plt.subplot(1, 2, 1)\n",
    "  axis.plot(range(1, epochs+1), losses_train, label='train')\n",
    "  axis.plot(range(1, epochs+1), losses_valid, label='valid')\n",
    "  axis.plot(range(1, epochs+1), losses_test, label='test')\n",
    "  \n",
    "  axis.legend()\n",
    "  axis.set_ylabel('Loss')\n",
    "  axis.set_xlabel('Epochs')\n",
    "  plt.title(('h_1 = ', h1, 'h_2 =', h2))\n",
    "  \n",
    "  \n",
    "  axis = plt.subplot(1, 2, 2)\n",
    "  axis.plot(range(1, epochs+1), accs_train, label='train')\n",
    "  axis.plot(range(1, epochs+1), accs_valid, label='valid')\n",
    "  axis.plot(range(1, epochs+1), accs_test, label='test')\n",
    "  \n",
    "  \n",
    "  axis.legend()\n",
    "  axis.set_ylabel('Accuracy')\n",
    "  axis.set_xlabel('Epochs')\n",
    "  plt.title(('h_1 = ', h1, 'h_2 =', h2))\n",
    "  \n",
    "  #axis.set_title('%d hidden units' % (h1,', ', h2))\n",
    "  \n",
    "  \n",
    "  \n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RlJOZ2o7I0tJ"
   },
   "source": [
    "**Building the model .**\n",
    "\n",
    "3. Training  the MLP using the probability loss (\n",
    "cross entropy\n",
    ") as training criterion.  \n",
    "\n",
    ".\n",
    "In this part we use sthocastic gradient descent to optimize the number of hidden unites to be between 0.5 M and 1M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1196
    },
    "colab_type": "code",
    "id": "ljfe59ICBm1k",
    "outputId": "e747b57d-9e98-4894-9b81-af3ad1a9f1d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy for h1=  500 h2 = 200 :  0.7943\n",
      "Validation accuracy for h1=  500 h2 = 300 :  0.6615\n",
      "Validation accuracy for h1=  800 h2 = 200 :  0.8416\n"
     ]
    }
   ],
   "source": [
    "#3. Training the model with sthocastic gradient descent\n",
    "\n",
    "zero=10**-15\n",
    "\n",
    "\n",
    "for i, h1 in enumerate([500, 800]):\n",
    "  \n",
    "  for j, h2 in enumerate ([200, 300]):\n",
    "  \n",
    "  \n",
    "    NN_SGD = NN(784, 10, hidden_dims=(h1, h2), initialization='glorot')\n",
    "    \n",
    "    # Model training \n",
    "    losses_train, accs_train = [], []\n",
    "    losses_valid, accs_valid = [], []\n",
    "    losses_test, accs_test = [], []\n",
    "\n",
    "    epochs=100\n",
    "\n",
    "    for epoch in range (epochs):\n",
    "  \n",
    "      loss = NN_SGD.train_mbatch(X_train, digit_y_train_onehot, mb_size=100)\n",
    "\n",
    "    #Test in validation and test set: test_mbatch(x_t, y_t_o, y_t)\n",
    "    \n",
    "      loss_train, acc_train = NN_SGD.test_mbatch(X_train, digit_y_train_onehot, y_train)\n",
    "      loss_valid, acc_valid = NN_SGD.test_mbatch(X_valid, digit_y_valid_onehot, y_valid) \n",
    "      \n",
    "    \n",
    "      losses_train.append(loss_train) \n",
    "      accs_train.append(acc_train)\n",
    "      losses_valid.append(loss_valid)\n",
    "      accs_valid.append(acc_valid)\n",
    "      \n",
    "    \n",
    "    #learning_curves(losses_train, accs_train, losses_valid, accs_valid,losses_test, accs_test, h1, h2)\n",
    "\n",
    "    print ('Validation accuracy for h1= ', h1, 'h2 =', h2, ': ', acc_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "62GmI4pVMFTo"
   },
   "source": [
    "### The best combination was h1=800, h2=200 with accuracy 84 %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SaGhxDQ1Bm1v"
   },
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mwGozQi3GYu1"
   },
   "outputs": [],
   "source": [
    "# Function to plot loss under different initialization weight types\n",
    "\n",
    "def graf_normalization (epochs, zeros_losses, normal_losses, glorot_losses):\n",
    "  fig = plt.figure()\n",
    "  plt.plot(range(epochs),  zeros_losses, label='zeros')\n",
    "  plt.plot(range(epochs), normal_losses, label='normal')\n",
    "  plt.plot(range(epochs), glorot_losses, label='glorot')\n",
    "\n",
    "  #plt.set_ylabel('Loss')\n",
    "  #plt.set_xlabel('Epochs')\n",
    "  plt.title(\"Initialization\")\n",
    "\n",
    "  plt.legend()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LYpn078PQevy"
   },
   "source": [
    "Training with initialization: zeros, normal, glorot. Epoch 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "colab_type": "code",
    "id": "AQ-9za__Bm1w",
    "outputId": "ad5932cb-3d4a-4e45-bbbf-979aa1f08bd9"
   },
   "outputs": [],
   "source": [
    "#1. Weigths initialization data from dataloader\n",
    "\n",
    "print('Weigths initialization')\n",
    "\n",
    "# Set timer\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "#training 10 epochs for each initialization type\n",
    "epochs=10\n",
    "\n",
    "zeros_losses=[]\n",
    "normal_losses=[]\n",
    "glorot_losses=[]\n",
    "weigths=['zeros', 'normal', 'glorot']\n",
    "\n",
    "for i, init in enumerate(weigths, 0):\n",
    "    \n",
    "    NN_digits= NN(784, 10, hidden_dims=(500,300), initialization=init)\n",
    "\n",
    "    for epoch in range (epochs): \n",
    "    \n",
    "        loss=NN_digits.train_mbatch(X_train, digit_y_train_onehot, mb_size=100)\n",
    "        if (init=='zeros'):\n",
    "            zeros_losses.append(loss)\n",
    "        if (init == 'normal'):\n",
    "            normal_losses.append(loss)\n",
    "        if (init == 'glorot'):\n",
    "            glorot_losses.append(loss)\n",
    "            \n",
    "time_mb = time.time() - start_time\n",
    "print('Time with minibatch gradient decent implementation: %f seconds\\n' % time_mb)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kkqRrc2dHY-7"
   },
   "outputs": [],
   "source": [
    "# Make graphic for different weigths initialization\n",
    "\n",
    "#graf_normalization (epochs, zeros_losses, normal_losses, glorot_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1RR2yNleBm12"
   },
   "source": [
    "# 2. Hyperparameter Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the best hyperparameters with validation dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LHdMuh_-Bm13"
   },
   "source": [
    "### Hyperparameter search function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V8GFsO_CBm16"
   },
   "outputs": [],
   "source": [
    "# Train the model using different hyperparameters, like mini-batch size, learning rate and epochs number\n",
    "# x_ds is the dataset to train, y_ds is the target dataset.\n",
    "def hyperparameter_checking(x_train_ds,y_train_ds,x_valid_ds,y_valid_ds,epochs,minibatch,learningrate,verbose=False):\n",
    "    x = x_train_ds\n",
    "    print('x.shape = ', x.shape)\n",
    "    y = y_train_ds\n",
    "    y=onehot(y,10)\n",
    "    print('y.shape = ', y.shape)\n",
    "    y_valid_onehot=onehot(y_valid_ds,10)\n",
    "    # input_dim, output_dim,hidden_dims,n_hidden=2,mode=',train',\n",
    "    # datapath=None,model_path=None\n",
    "    NN_mbatch_1= NN(784, 10, hidden_dims=(500,300),initialization='glorot')\n",
    "    loss_training_arr=[]\n",
    "    loss_validation_arr=[]\n",
    "    loss_mbatch_1=0\n",
    "    for epoch in range (epochs): \n",
    "        #x, y_onehot, mb_size=100, learning_rate=1e-1, weight_decay=0\n",
    "        loss_mbatch_1=NN_mbatch_1.train_mbatch(x, y, mb_size=minibatch,learning_rate=learningrate)\n",
    "        if verbose:\n",
    "            print('epoch ', epoch, ' loss ', loss_mbatch_1)\n",
    "        loss,accuracy=NN_mbatch_1.test_mbatch(x_valid_ds,y_valid_onehot,y_valid_ds)\n",
    "        loss_training_arr.append(loss_mbatch_1)\n",
    "        loss_validation_arr.append(loss)\n",
    "    print('epoch ', epoch, ' loss ', loss_mbatch_1)\n",
    "    return loss_training_arr,loss_validation_arr,accuracy,NN_mbatch_1 # accuracy of validation test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "# Display differents graphs using different hyperparameters saved in arrays.\n",
    "# Input variables are arrays with the same lenght, with the values to graph\n",
    "\n",
    "def show_graphs(minibatch_arr,epoch_arr,learningrate_arr,loss_arr):\n",
    "# define a list of markevery cases to plot\n",
    "    cases=[]\n",
    "    x_values=[]\n",
    "    y_values=[]\n",
    "    for i in range(len(minibatch_arr)):\n",
    "        cases.append([minibatch_arr[i],epoch_arr[i],learningrate_arr[i]])\n",
    "        # define the data for cartesian plots\n",
    "        x_values.append(range(epoch_arr[i]))\n",
    "        y_values.append(loss_arr[i])\n",
    "    # define the figure size and grid layout properties\n",
    "    figsize = (12, 10)\n",
    "    cols = 2\n",
    "    gs = gridspec.GridSpec(len(cases) // cols + 1, cols)\n",
    "    gs.update(hspace=0.4)\n",
    "    \n",
    "    fig1 = plt.figure(num=1, figsize=figsize)\n",
    "    ax = []\n",
    "    for i, case in enumerate(cases):\n",
    "        row = (i // cols)\n",
    "        col = i % cols\n",
    "        ax.append(fig1.add_subplot(gs[row, col]))\n",
    "        ax[-1].set_title('Minibatch=%s, Epoch=%s, Learning=%s' % (str(case[0]),str(case[1]),str(case[2])))\n",
    "        ax[-1].plot(x_values[i], y_values[i], 'o', ls='-', ms=4, markevery=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using different hyperparameters with validation set and Glorot initialization:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## * Changing Mini-batch size:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With different Mini-batch sizes, Epoch=100, Learning rate=0.1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ds=X_train\n",
    "y_ds=y_train\n",
    "x_valid_ds=X_valid\n",
    "y_valid_ds=y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_arr_0=[] #training loss array\n",
    "loss_arr_valid_0=[] #valid loss array\n",
    "epoch_arr_0=[] #epoch array (save the epoch used on each mini-batch)\n",
    "acc_arr_0=[] # accuracy with validation set array \n",
    "lr_0=1e-1 #learning rate used.\n",
    "epoch_0=50 #total epoch \n",
    "for i, minibatch in enumerate([50,100,150,500]):\n",
    "    loss_array,loss_array_valid,acc_valid,nn_model=hyperparameter_checking(x_ds,y_ds,x_valid_ds,y_valid_ds,epoch_0,minibatch,lr_0)\n",
    "    loss_arr_0.append(loss_array)\n",
    "    loss_arr_valid_0.append(loss_array_valid)\n",
    "    epoch_arr_0.append(epoch_0)\n",
    "    acc_arr_0.append(acc_valid)\n",
    "    print('Accuracy: %s' % str(acc_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_arr_0=[50,50,50,50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minibatch_arr=[50,100,150,500]\n",
    "fig, ax = plt.subplots(1,3,figsize=(18, 5))\n",
    "# plt.xlabel('Epoch')\n",
    "ax[2].plot(minibatch_arr, acc_arr_0, markevery=1)\n",
    "ax[2].set_xlabel('Mini-Batch')\n",
    "ax[2].set_ylabel('Accuracy')\n",
    "ax[2].set_title('Accuracy with different Mini-batch size')\n",
    "ax[2].grid(True)\n",
    "ax[0].plot(range(epoch_arr_0[3]), loss_arr_0[3], label='Mini-batch: %s' % str(minibatch_arr[3]))\n",
    "ax[0].plot(range(epoch_arr_0[2]), loss_arr_0[2], label='Mini-batch: %s' % str(minibatch_arr[2]))\n",
    "ax[0].plot(range(epoch_arr_0[1]), loss_arr_0[1], label='Mini-batch: %s' % str(minibatch_arr[1]))\n",
    "ax[0].plot(range(epoch_arr_0[0]), loss_arr_0[0], label='Mini-batch: %s' % str(minibatch_arr[0]))\n",
    "ax[0].legend()\n",
    "ax[0].set_xlabel('Epoch')\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[0].set_title('Total Loss per Epoch - Training Set')\n",
    "ax[1].plot(range(epoch_arr_0[3]), loss_arr_valid_0[3], label='Mini-batch: %s' % str(minibatch_arr[3]))\n",
    "ax[1].plot(range(epoch_arr_0[2]), loss_arr_valid_0[2], label='Mini-batch: %s' % str(minibatch_arr[2]))\n",
    "ax[1].plot(range(epoch_arr_0[1]), loss_arr_valid_0[1], label='Mini-batch: %s' % str(minibatch_arr[1]))\n",
    "ax[1].plot(range(epoch_arr_0[0]), loss_arr_valid_0[0], label='Mini-batch: %s' % str(minibatch_arr[0]))\n",
    "ax[1].legend()\n",
    "ax[1].set_xlabel('Epoch')\n",
    "ax[1].set_ylabel('Loss')\n",
    "ax[1].set_title('Total Loss per Epoch - Validation Set')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learningrate_arr=[lr_0,lr_0,lr_0,lr_0]\n",
    "show_graphs(minibatch_arr,epoch_arr_0,learningrate_arr,loss_arr_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We obtain a better accuracy when we choose a smaller Mini-Batch size. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## * Changing Learning-Rate:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With different Learning Rates, Epoch=100, Minibath size=50:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ds=X_train\n",
    "y_ds=y_train\n",
    "x_valid_ds=X_valid\n",
    "y_valid_ds=y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_arr_1=[] #learning rate array.\n",
    "loss_arr_1=[]   #training loss array\n",
    "loss_arr_valid_1=[] #valid loss array\n",
    "epoch_arr_1=[] #epoch array (save the epoch used on each mini-batch)\n",
    "acc_arr_1=[] # accuracy with validation set array \n",
    "epoch_1=50  #total epoch \n",
    "mb=50\n",
    "for i, learningrate in enumerate([0.1,0.01,0.001]):\n",
    "    loss_array,loss_array_valid,acc_valid,nn_model=hyperparameter_checking(x_ds,y_ds,x_valid_ds,y_valid_ds,epoch_1,mb,learningrate)\n",
    "    loss_arr_1.append(loss_array)\n",
    "    loss_arr_valid_1.append(loss_array_valid)\n",
    "    epoch_arr_1.append(epoch_1)\n",
    "    acc_arr_1.append(acc_valid)\n",
    "    print('Accuracy: %s' % str(acc_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learningrate_arr=[0.1,0.01,0.001]\n",
    "fig, ax = plt.subplots(1,3,figsize=(18, 5))\n",
    "# plt.xlabel('Epoch')\n",
    "ax[2].plot(learningrate_arr, acc_arr_1, markevery=1)\n",
    "ax[2].set_xlabel('Learning Rate')\n",
    "ax[2].set_ylabel('Accuracy')\n",
    "ax[2].set_title('Accuracy with different Learning Rate')\n",
    "ax[2].grid(True)\n",
    "ax[0].plot(range(epoch_arr_1[2]), loss_arr_1[2], label='Learning Rate: %s' % str(learningrate_arr[2]))\n",
    "ax[0].plot(range(epoch_arr_1[1]), loss_arr_1[1], label='Learning Rate: %s' % str(learningrate_arr[1]))\n",
    "ax[0].plot(range(epoch_arr_1[0]), loss_arr_1[0], label='Learning Rate: %s' % str(learningrate_arr[0]))\n",
    "ax[0].legend()\n",
    "ax[0].set_xlabel('Epoch')\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[0].set_title('Total Loss per Epoch - Training Set')\n",
    "ax[1].plot(range(epoch_arr_1[2]), loss_arr_valid_1[2], label='Learning Rate: %s' % str(learningrate_arr[2]))\n",
    "ax[1].plot(range(epoch_arr_1[1]), loss_arr_valid_1[1], label='Learning Rate: %s' % str(learningrate_arr[1]))\n",
    "ax[1].plot(range(epoch_arr_1[0]), loss_arr_valid_1[0], label='Learning Rate: %s' % str(learningrate_arr[0]))\n",
    "ax[1].legend()\n",
    "ax[1].set_xlabel('Epoch')\n",
    "ax[1].set_ylabel('Loss')\n",
    "ax[1].set_title('Total Loss per Epoch - Validation Set')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minibatch_arr=[50,50,50]\n",
    "show_graphs(minibatch_arr,epoch_arr_1,learningrate_arr,loss_arr_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We get a better accuracy if we choose a  lower learning rate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## * Changing Epochs:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With different Epochs, Learning Rates=0.1, Minibath size=50:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ds=X_train\n",
    "y_ds=y_train\n",
    "x_valid_ds=X_valid\n",
    "y_valid_ds=y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_arr_2=[] #learning rate array.\n",
    "loss_arr_2=[]   #training loss array\n",
    "loss_arr_valid_2=[] #valid loss array\n",
    "epoch_arr_2=[] #epoch array (save the epoch used on each mini-batch)\n",
    "acc_arr_2=[] # accuracy with validation set array \n",
    "learning_rate=0.1\n",
    "mb=50\n",
    "for i, epochs in enumerate([30,50,80,100]):\n",
    "    loss_array,loss_array_valid,acc_valid,nn_model=hyperparameter_checking(x_ds,y_ds,x_valid_ds,y_valid_ds,epochs,mb,learning_rate)\n",
    "    loss_arr_2.append(loss_array)\n",
    "    loss_arr_valid_2.append(loss_array_valid)\n",
    "    epoch_arr_2.append(epochs)\n",
    "    acc_arr_2.append(acc_valid)\n",
    "    print('Accuracy: %s' % str(acc_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_arr=[30,50,80,100]\n",
    "fig, ax = plt.subplots(1,3,figsize=(18, 5))\n",
    "# plt.xlabel('Epoch')\n",
    "ax[2].plot(epochs_arr, acc_arr_2, markevery=1)\n",
    "ax[2].set_xlabel('Epoch')\n",
    "ax[2].set_ylabel('Accuracy')\n",
    "ax[2].set_title('Accuracy with different Epochs')\n",
    "ax[2].grid(True)\n",
    "ax[0].plot(range(epoch_arr_2[3]), loss_arr_2[3], label='Epoch: %s' % str(epochs_arr[3]))\n",
    "ax[0].plot(range(epoch_arr_2[2]), loss_arr_2[2], label='Epoch: %s' % str(epochs_arr[2]))\n",
    "ax[0].plot(range(epoch_arr_2[1]), loss_arr_2[1], label='Epoch: %s' % str(epochs_arr[1]))\n",
    "ax[0].plot(range(epoch_arr_2[0]), loss_arr_2[0], label='Epoch: %s' % str(epochs_arr[0]))\n",
    "ax[0].legend()\n",
    "ax[0].set_xlabel('Epoch')\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[0].set_title('Total Loss per Epoch - Training Set')\n",
    "ax[1].plot(range(epoch_arr_2[3]), loss_arr_valid_2[3], label='Epoch: %s' % str(epochs_arr[3]))\n",
    "ax[1].plot(range(epoch_arr_2[2]), loss_arr_valid_2[2], label='Epoch: %s' % str(epochs_arr[2]))\n",
    "ax[1].plot(range(epoch_arr_2[1]), loss_arr_valid_2[1], label='Epoch: %s' % str(epochs_arr[1]))\n",
    "ax[1].plot(range(epoch_arr_2[0]), loss_arr_valid_2[0], label='Epoch: %s' % str(epochs_arr[0]))\n",
    "ax[1].legend()\n",
    "ax[1].set_xlabel('Epoch')\n",
    "ax[1].set_ylabel('Loss')\n",
    "ax[1].set_title('Total Loss per Epoch - Validation Set')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minibatch_arr=[50,50,50,50]\n",
    "show_graphs(minibatch_arr,epoch_arr_1,learningrate_arr,loss_arr_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The best Accuracy was found with Epoch=80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The best hyperparameters to train the model are Mini-batch size: 50 and Learning Rate: 0.1 and Epoch:80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model with Mini-batch size=50, Epoch=80 and Learning rate=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_arr=[]\n",
    "epochs=80\n",
    "mb=50\n",
    "lr=1e-1\n",
    "x=x_train\n",
    "y=onehot(y_train,10)\n",
    "# input_dim, output_dim,hidden_dims,n_hidden=2,mode=',train',\n",
    "# datapath=None,model_path=None\n",
    "NN_mbatch_1= NN(784, 10, hidden_dims=(500,300),initialization='glorot')\n",
    "for epoch in range (epochs): \n",
    "    #x, y_onehot, mb_size=100, learning_rate=1e-1, weight_decay=0\n",
    "    loss_mbatch_1=NN_mbatch_1.train_mbatch(x, y, mb_size=50,learning_rate=0.1)\n",
    "    print('epoch ', epoch, ' loss ', loss_mbatch_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The accuracy obtained by the model using the training set is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=NN_mbatch_1.prediction_mbatch(x_train)\n",
    "accuracy=NN_mbatch_1.accuracy_mbatch(y_pred,y_train)\n",
    "print('Accuracy: %s' % str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The accuracy obtained by the model using the testing set is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test=NN_mbatch_1.prediction_mbatch(X_test)\n",
    "accuracy=NN_mbatch_1.accuracy_mbatch(y_pred_test,Y_test)\n",
    "print('Accuracy: %s' % str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zCRCdN08Bm17",
    "outputId": "d88223e6-6776-41bd-bf74-99ae7875096e"
   },
   "outputs": [],
   "source": [
    "# testing for different h1 size\n",
    "\n",
    "for i, h1 in enumerate([500, 600, 800, 1000]):\n",
    "    print(h1)\n",
    "    \n",
    "    NN_mbatch_1 = NN(784, 10, hidden_dims=(h1,300), initialization='glorot')\n",
    "    \n",
    "    losses_train, acc_train, losses_valid, acc_valid, NN_mbatch_1 = hyperparameter_search(\n",
    "        NN_mbatch_1, X_train, y_train, digit_y_train_onehot, X_valid, y_valid, digit_y_valid_onehot, \n",
    "        10, 100, 1e-1, verbose=False)\n",
    "    \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7bVsBHqtBm2A"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZNXG39GyBm2D"
   },
   "source": [
    "# 3. Finite difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n-NMuZU7Bm2E"
   },
   "outputs": [],
   "source": [
    "#Approximation of the gradient of the loos at the end of training, with respect to W3 \n",
    "# (the second layer weigths) with to the first p = min(10;m) elements of W3.\n",
    "\n",
    "\n",
    "#function to calculate the finite difference for  \n",
    "\n",
    "def loop_finite_diff(self, x, y, epsilon=1e-5):\n",
    "        a1, h1, a2, h2, oa, os = self.forward(x)\n",
    "        grads = self.backward(x, y, a1, h1, a2, h2, oa, os)\n",
    "        loss = self.loss(y, os)\n",
    "        \n",
    "        grads_finite_diff = []\n",
    "        \n",
    "        for p in self.parameters[0]:\n",
    "            grad_fdiff = np.zeros(shape=p.shape)\n",
    "            for i, v in np.ndenumerate(p):\n",
    "                p[i] += epsilon\n",
    "                _, _, _, _, _, os = self.forward(x)\n",
    "                loss_diff = self.loss(os, y)\n",
    "                grad_fdiff[index] = (loss_diff - loss) / epsilon\n",
    "                p[index] -= epsilon\n",
    "            grads_finite_diff.append(grad_fdiff)\n",
    "        return gradients_finite_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M2C9wE9VBm2G",
    "outputId": "337fde68-a722-4702-ff9b-83eccd8a58f5"
   },
   "outputs": [],
   "source": [
    "#1. epsilon = 1 / N\n",
    "\n",
    "#Use at least 5 values of N from the set {k10**i : i E {0; : : : ; 5g} k E {1, 5}}\n",
    "epsilon=[]\n",
    "N = []\n",
    "for exp in range (1, 6, 2):\n",
    "    a= 10**exp\n",
    "    N.append(a)\n",
    "for exp in range (1, 5, 2):\n",
    "    b= 5*10**exp\n",
    "    N.append(b)\n",
    "for i in range (5):\n",
    "    epsilon.append(1/N[i])\n",
    "\n",
    "print (N)\n",
    "print (epsilon)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5CDfE7CDBm2J"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PQGlR_zYBm2M"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copie de assignement_1_IFT6135_new2-Copy3.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
