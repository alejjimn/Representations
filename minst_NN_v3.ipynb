{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Hyperparameter Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Mnist dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Charging data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charging and shuffling training data\n",
    "import csv\n",
    "def get_ds(ds_file):\n",
    "    '''Open train and test datasets, read them and, store their records to a list'''\n",
    "    with open (ds_file, 'r') as t_file:\n",
    "        ds = csv.reader(t_file)\n",
    "        records = []\n",
    "        for line in ds:\n",
    "            records.append([float(i) for i in line]) # list comprehension to convert lists' strings to ints on the fly        \n",
    "    return records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get our datasets\n",
    "train_data = get_ds('data/train_x.csv')\n",
    "train_labels=get_ds('data/train_y.csv')\n",
    "test_data=get_ds('data/test_x.csv')\n",
    "test_labels = get_ds('data/test_y.csv')\n",
    "valid_data=get_ds('data/val_x.csv')\n",
    "valid_labels = get_ds('data/val_y.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=np.asarray(train_data)\n",
    "Y_train=np.asarray(train_labels)\n",
    "X_test=np.asarray(test_data)\n",
    "Y_test=np.asarray(test_labels)\n",
    "X_valid=np.asarray(valid_data)\n",
    "Y_valid=np.asarray(valid_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Normalization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.trunc(X_train/255).astype(int)\n",
    "x_valid = np.trunc(X_valid/255).astype(int)\n",
    "x_test = np.trunc(X_test/255).astype(int)\n",
    "y_train = np.trunc(Y_train).astype(int)\n",
    "y_valid = np.trunc(Y_valid).astype(int)\n",
    "y_test = np.trunc(Y_test).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 784) (50000, 1)\n",
      "(10000, 784) (10000, 1)\n",
      "(10000, 784) (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape,y_train.shape)\n",
    "print(x_valid.shape,y_valid.shape)\n",
    "print(x_test.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset test:  \n",
    "#### Printing the first 3 labels and 3 entries in the training set to validate they match the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label = [5.0]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label = [0.0]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label = [4.0]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#reshape gives a new shape to an array without changing its data.\n",
    "for i in range(0,3):\n",
    "    trainsetmtx = np.reshape(train_data[i], (28,28))\n",
    "    imgplot = plt.imshow(trainsetmtx, cmap=plt.cm.get_cmap(\"gray_r\"))\n",
    "    print ('Label = %s' % train_labels[i]), plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This class implementation is inspired from the NN implemented in cours IFT6093\n",
    "class NN(object):\n",
    "    \n",
    "    \n",
    "    def __init__(self, input_dim, output_dim,hidden_dims=(1024,2048),n_hidden=2, initialization='zeros', mode=',train',\n",
    "                 datapath=None,model_path=None):\n",
    "        \n",
    "        self.indim = input_dim\n",
    "        self.hd1 = hidden_dims[0] \n",
    "        self.hd2 = hidden_dims[1]\n",
    "        self.n_hidden = n_hidden\n",
    "        self.outd = output_dim\n",
    "        self.W1 = np.zeros(shape=(hidden_dims[0], input_dim))\n",
    "        #print('W1.shape =', self.W1.shape)\n",
    "        #print('W1 = ', self.W1)\n",
    "        #print('\\n')\n",
    "        \n",
    "        self.b1 = np.zeros(hidden_dims[0])\n",
    "        #print('b1.shape =', self.b1.shape)\n",
    "        #print('b1 = ', self.b1)\n",
    "        #print('\\n')\n",
    "        \n",
    "        self.W2 = np.zeros(shape=(hidden_dims[1], hidden_dims[0]))\n",
    "        #print('W2.shape =', self.W2.shape)\n",
    "        #print('W2 = ', self.W2)\n",
    "        #print('\\n')\n",
    "        \n",
    "        self.b2 = np.zeros(hidden_dims[1])\n",
    "        #print('b2.shape =', self.b2.shape)\n",
    "        #print('b2 = ', self.b2)\n",
    "        #print('\\n')\n",
    "        \n",
    "        self.W3 = np.zeros(shape=(output_dim, hidden_dims[1]))\n",
    "        #print('W3.shape =', self.W3.shape)\n",
    "        #print('W3 = ', self.W3)\n",
    "        #print('\\n')\n",
    "        \n",
    "        self.b3 = np.zeros(output_dim)\n",
    "        #print('b3.shape =', self.b3.shape)\n",
    "        #print('b3 = ', self.b3)\n",
    "        #print('\\n')\n",
    "        \n",
    "        if initialization=='normal':\n",
    "            self.initialize_weights_normal()\n",
    "            #print('W1 = ', self.W1)\n",
    "            #print('W2 = ', self.W2)\n",
    "            #print('W3 = ', self.W3)\n",
    "            \n",
    "            \n",
    "        if initialization=='glorot':\n",
    "            self.initialize_weights_glorot()\n",
    "            #print('W1 = ', self.W1)\n",
    "            #print('W2 = ', self.W2)\n",
    "            #print('W3 = ', self.W3)\n",
    "            \n",
    "        \n",
    "        self.parameters = [self.W3, self.b3, self.W2, self.b2, self.W1, self.b1]\n",
    "        \n",
    "        \n",
    "    def initialize_weights_normal(self):\n",
    "        \n",
    "        self.W1 = np.random.standard_normal(size=(self.hd1, self.indim))\n",
    "        self.W2 = np.random.standard_normal(size=(self.hd2, self.hd1))\n",
    "        self.W3 = np.random.standard_normal(size=(self.outd, self.hd2))\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def initialize_weights_glorot(self):\n",
    "        \n",
    "        dl1 = np.sqrt(6/(self.indim + self.hd1))\n",
    "        dl2 = np.sqrt(6/(self.hd1 + self.hd2))\n",
    "        dl3 = np.sqrt(6/(self.hd2 + self.outd))\n",
    "        self.W1 = np.random.uniform(low=(-dl1), high=dl1, size=(self.hd1, self.indim))\n",
    "        self.W2 = np.random.uniform(low=(-dl2), high=dl2, size=(self.hd2, self.hd1))\n",
    "        self.W3 = np.random.uniform(low=(-dl3), high=dl3, size=(self.outd, self.hd2))\n",
    "        \n",
    "        return self\n",
    "        \n",
    "        \n",
    "        \n",
    "    #Method inspired from NN implemented in cours IFT6093\n",
    "    def activation (self,input):\n",
    "        return (input > 0) * input  \n",
    "    \n",
    "    #line 85\n",
    "\n",
    "    def forward(self,x):\n",
    "        #print('forward')\n",
    "        \n",
    "        a1 = np.dot (self.W1, x) + self.b1 \n",
    "        #print('a1 = np.dot (self.W1, x) + self.b1')\n",
    "        #print('a1.shape =', a1.shape)\n",
    "        #print('a1 = ', a1)\n",
    "        #print('\\n')\n",
    "        \n",
    "        h1 = self.activation (a1)\n",
    "        #print('h1 = self.activation (a1)')\n",
    "        #print('h1.shape =', h1.shape)\n",
    "        #print('h1 = ', h1)\n",
    "        #print('\\n')\n",
    "        \n",
    "        a2 = np.dot (self.W2, h1) + self.b2\n",
    "        #print('a2 = np.dot (self.W2, h1) + self.b2')\n",
    "        #print('a2.shape =', a2.shape)\n",
    "        #print('a2 = ', a2)\n",
    "        #print('\\n')\n",
    "        \n",
    "        h2 = self.activation (a2)\n",
    "        #print('h2 = self.activation (a2)')\n",
    "        #print('h2.shape =', h2.shape)\n",
    "        #print('h2 = ', h2)\n",
    "        #print('\\n')\n",
    "        \n",
    "    \n",
    "        oa = np.dot (self.W3, h2) + self.b3\n",
    "        #print('oa = np.dot (self.W3, h2) + self.b3')\n",
    "        #print('oa.shape =', oa.shape)\n",
    "        #print('oa = ', oa)\n",
    "        #print('\\n')\n",
    "        \n",
    "        os = self.softmax (oa, axis=0)\n",
    "        #print('os = softmax (oa)')\n",
    "        #print('os.shape =', os.shape)\n",
    "        #print('os = ', os)\n",
    "        #print('\\n')\n",
    "               \n",
    "        return a1, h1, a2, h2, oa, os\n",
    "    \n",
    "    \n",
    "\n",
    "    #Methods inspired from NN implemented in cours IFT6093\n",
    "    def loss (self, y, os):\n",
    "        return (y * (-np.log(os))).sum()\n",
    "    \n",
    "\n",
    "    def softmax (self,x,axis=1):\n",
    "        shiftx = x - np.max (x, axis=axis, keepdims=True)\n",
    "        exps = np.exp (shiftx)\n",
    "        y = exps / exps.sum (axis=axis, keepdims=True)\n",
    "        return y\n",
    "\n",
    "\n",
    "    def backward(self, x, y, a1, h1, a2, h2, oa, os, weight_decay=0, cache=None):\n",
    "        #print ('backward')\n",
    "        #print('x.shape = ', x.shape)\n",
    "        #print('y.shape = ', y.shape)\n",
    "        #print('os.shape = ', os.shape)\n",
    "        grad_oa = os - y\n",
    "        #print('grad_oa.shape =', grad_oa.shape)\n",
    "        #print('\\n')\n",
    "        \n",
    "        grad_W3 = np.outer (grad_oa, h2) + weight_decay * self.W3\n",
    "        #print('grad_W3.shape =', grad_W3.shape)\n",
    "        #print('\\n')\n",
    "        \n",
    "        grad_b3 = grad_oa\n",
    "        #print('grad_b3.shape =', grad_b3.shape)\n",
    "        #print('\\n')\n",
    "        \n",
    "        grad_h2 = np.dot (self.W3.T, grad_oa)\n",
    "        #print(' grad_h2.shape =', grad_h2.shape)\n",
    "        #print('\\n')\n",
    "        \n",
    "        grad_a2 = (a2 > 0) * grad_h2\n",
    "        #print('grad_a2.shape =', grad_a2.shape)\n",
    "        #print('\\n')\n",
    "        \n",
    "        grad_W2 = np.outer (grad_a2, h1) + weight_decay * self.W2\n",
    "        #print('grad_W2.shape =', grad_W2.shape)\n",
    "        #print('\\n')\n",
    "        \n",
    "        grad_b2 = grad_a2 \n",
    "        #print('grad_b2.shape =', grad_b2.shape)\n",
    "        #print('\\n')\n",
    "        \n",
    "        grad_h1 = np.dot (self.W2.T, grad_a2)\n",
    "        #print('grad_h1.shape =', grad_h1.shape)\n",
    "        #print('\\n')\n",
    "        \n",
    "        grad_a1 = (a1 > 0) * grad_h1\n",
    "        #print('grad_a1.shape =', grad_a1.shape)\n",
    "        #print('\\n')\n",
    "        \n",
    "        grad_W1 = np.outer (grad_a1, x) + weight_decay * self.W1\n",
    "        #print('grad_W1.shape =', grad_W1.shape)\n",
    "        #print('\\n')\n",
    "        \n",
    "        grad_b1 = grad_a1\n",
    "        #print('grad_b1.shape =', grad_b1.shape)\n",
    "        #print('\\n')\n",
    "        \n",
    "        grads=[grad_W3, grad_b3, grad_W2, grad_b2, grad_W1, grad_b1]\n",
    "   \n",
    "        return grads\n",
    "\n",
    "\n",
    "\n",
    "    def update(self, grads, learning_rate):\n",
    "        for p, grad in zip(self.parameters, grads):\n",
    "            p -= learning_rate * grad\n",
    "        \n",
    "    #line 201   \n",
    "\n",
    "    def train_SGD(self, x, y_onehot, n, learning_rate=1e-1, weight_decay=0):\n",
    "        y= y_onehot\n",
    "        #print('x.shape = ', x.shape)\n",
    "        #print('y.shape = ', y.shape)\n",
    "        losses = 0\n",
    "        if (n==1):\n",
    "            a1, h1, a2, h2, oa, os = self.forward(x)\n",
    "            grads = self.backward(x, y, a1, h1, a2, h2, oa, os)\n",
    "            self.update(grads, learning_rate)\n",
    "            loss = self.loss(y, os)\n",
    "            losses += loss  \n",
    "            average_loss = losses / n\n",
    "        else:    \n",
    "            for j in range(x.shape[0]):\n",
    "                a1, h1, a2, h2, oa, os = self.forward(x[j])\n",
    "                grads = self.backward(x[j], y[j], a1, h1, a2, h2, oa, os)\n",
    "                self.update(grads, learning_rate)\n",
    "                loss = self.loss(y[j], os)\n",
    "                losses += loss \n",
    "                \n",
    "            average_loss = losses / n\n",
    "            #print (average_loss)\n",
    "\n",
    "        #print (average_loss)   \n",
    "        return average_loss\n",
    "    \n",
    "    \n",
    "    def prediction_SGD (self, x):\n",
    "        predictions = np.zeros(x.shape[0])\n",
    "        for i in range(x.shape[0]):\n",
    "            _, _, _, _, _, os = self.forward(x[i])\n",
    "            predictions[i] = os.argmax()\n",
    "            \n",
    "        return predictions\n",
    "    \n",
    "    def accuracy_SGD (self, prediction, y):\n",
    "        accuracies=0\n",
    "        for i in range (y.shape[0]):\n",
    "            accuracies+=(prediction[i]==y[i])\n",
    "            \n",
    "        return accuracies / y.shape[0]\n",
    "    \n",
    "    \n",
    "    def test_SGD(self, x, y_onehot, y):\n",
    "        pred=np.zeros(y.shape[0])\n",
    "        avg_loss=0\n",
    "        for i in range (x.shape[0]):\n",
    "            _, _, _, _, _, os = self.forward(x[i])\n",
    "            loss=self.loss (y_onehot[i], os)\n",
    "            avg_loss+=loss\n",
    "            pred[i]=os.argmax()\n",
    "            \n",
    "        accuracy=self.accuracy_SGD(pred, y)    \n",
    "        return avg_loss / x.shape[0] , accuracy\n",
    "    \n",
    "   \n",
    "    def forward_mbatch(self, x):\n",
    "        #print ('forward minibtach')\n",
    "        a1 = np.dot ( x, self.W1.T) + self.b1 \n",
    "        #print('a1 = np.dot (x, self.W1.T) + self.b1')\n",
    "        #print('a1.shape =', a1.shape)\n",
    "        #print('a1 = ', a1)\n",
    "        #print('\\n')\n",
    "        \n",
    "        h1 = self.activation (a1)\n",
    "        #print('h1 = self.activation (a1)')\n",
    "        #print('h1.shape =', h1.shape)\n",
    "        #print('h1 = ', h1)\n",
    "        #print('\\n')\n",
    "        \n",
    "        a2 = np.dot (h1, self.W2.T) + self.b2\n",
    "        #print('a2 = np.dot (h1, self.W2.T) + self.b2')\n",
    "        #print('a2.shape =', a2.shape)\n",
    "        #print('a2 = ', a2)\n",
    "        #print('\\n')\n",
    "        \n",
    "        h2 = self.activation (a2)\n",
    "        #print('h2 = self.activation (a2)')\n",
    "        #print('h2.shape =', h2.shape)\n",
    "        #print('h2 = ', h2)\n",
    "        #print('\\n')\n",
    "        \n",
    "        oa = np.dot (h2, self.W3.T) + self.b3\n",
    "        #print('oa = np.dot (h2, self.W3.T) + self.b3')\n",
    "        #print('oa.shape =', oa.shape)\n",
    "        #print('oa = ', oa)\n",
    "        #print('\\n')\n",
    "        \n",
    "        os = self.softmax (oa, axis=1)\n",
    "        #print('os = softmax (oa)')\n",
    "        #print('os.shape =', os.shape)\n",
    "        #print('os = ', os)\n",
    "        #print('\\n')\n",
    "               \n",
    "        return a1, h1, a2, h2, oa, os\n",
    "    \n",
    "    #line 303\n",
    "        \n",
    "    def backward_mbatch(self, x, y, a1, h1, a2, h2, oa, os, batch_n, weight_decay=0):\n",
    "        #print ('backward minibatch')\n",
    "        \n",
    "        #print('x.shape = ', x.shape)\n",
    "        #print('y.shape = ', y.shape)\n",
    "        #print('os.shape = ', os.shape)\n",
    "        \n",
    "        \n",
    "        batch_n = x.shape[0]\n",
    "        bgrad_oa = os - y\n",
    "        #print('bgrad_oa.shape =', bgrad_oa.shape)\n",
    "        #print('\\n')\n",
    "        \n",
    "        bgrad_W3 = np.dot (bgrad_oa.T, h2) / batch_n  + weight_decay * self.W3\n",
    "        #print('bgrad_W3.shape =', bgrad_W3.shape)\n",
    "        #print('\\n')\n",
    "        \n",
    "        bgrad_b3 = bgrad_oa.mean(axis=0)\n",
    "        #print('bgrad_b3.shape =', bgrad_b3.shape)\n",
    "        #print('\\n')\n",
    "        \n",
    "        bgrad_h2 = np.dot (bgrad_oa, self.W3)\n",
    "        #print(' bgrad_h2.shape =', bgrad_h2.shape)\n",
    "        #print('\\n')\n",
    "        \n",
    "        bgrad_a2 = (a2 > 0) * bgrad_h2\n",
    "        #print('bgrad_a2.shape =', bgrad_a2.shape)\n",
    "        #print('\\n')\n",
    "        \n",
    "        bgrad_W2 = np.dot (bgrad_a2.T, h1) / batch_n  + weight_decay * self.W2\n",
    "        #print('bgrad_W2.shape =', bgrad_W2.shape)\n",
    "        #print('\\n')\n",
    "        \n",
    "        bgrad_b2 = bgrad_a2.mean(axis=0) \n",
    "        #print('bgrad_b2.shape =', bgrad_b2.shape)\n",
    "        #print('\\n')\n",
    "        \n",
    "        bgrad_h1 = np.dot (bgrad_a2, self.W2)\n",
    "        #print('bgrad_h1.shape =', bgrad_h1.shape)\n",
    "        #print('\\n')\n",
    "        \n",
    "     \n",
    "        bgrad_a1 = (a1 > 0) * bgrad_h1\n",
    "        #print('bgrad_a1.shape =', bgrad_a1.shape)\n",
    "        #print('\\n')\n",
    "        \n",
    "        bgrad_W1 = np.dot (bgrad_a1.T, x) / batch_n  + weight_decay * self.W1\n",
    "        #print('bgrad_W1.shape =', bgrad_W1.shape)\n",
    "        #print('\\n')\n",
    "        \n",
    "        bgrad_b1 = bgrad_a1.mean(axis=0)\n",
    "        #print('bgrad_b1.shape =', bgrad_b1.shape)\n",
    "        #print('\\n')\n",
    "        \n",
    "        bgrads=[bgrad_W3, bgrad_b3, bgrad_W2, bgrad_b2, bgrad_W1, bgrad_b1]\n",
    "   \n",
    "        return bgrads\n",
    "\n",
    "    #line 360\n",
    "\n",
    "    #Method taken fron homwork 3 in cours IFT6093\n",
    "    def loss_mbatch(self, os, y):\n",
    "        return (y * (-np.log(os))).sum(axis=1).mean(axis=0)     \n",
    "        \n",
    "    \n",
    "    #training with minibatch gradient decent\n",
    "    def train_mbatch(self, x, y_onehot, mb_size=100, learning_rate=1e-1, weight_decay=0):\n",
    "        average_loss=0\n",
    "        for i in range (0, x.shape[0], mb_size):\n",
    "            #print (i)\n",
    "            xi = x[i:(i+mb_size)]\n",
    "            yi = y_onehot[i:(i+mb_size)]\n",
    "        \n",
    "            losses = 0\n",
    "            a1, h1, a2, h2, oa, os = self.forward_mbatch(xi)\n",
    "            grads = self.backward_mbatch (xi, yi,a1, h1, a2, h2,oa, os, mb_size)\n",
    "            self.update(grads, learning_rate)\n",
    "            average_loss = self.loss_mbatch(os, yi) \n",
    "                          \n",
    "        return average_loss\n",
    "    \n",
    "    \n",
    "    #line 385\n",
    "    \n",
    "    def prediction_mbatch (self, x):\n",
    "        _, _, _, _, _, os = self.forward_mbatch(x)\n",
    "        return os.argmax(axis=1)\n",
    "    \n",
    "\n",
    "    def accuracy_mbatch (self, prediction, y):\n",
    "        accuracy = np.zeros(y.shape[0])\n",
    "        accuracy = prediction == y\n",
    "        return accuracy.mean(axis=0)\n",
    "    \n",
    "\n",
    "    def test_mbatch(self, x, y_onehot, y):\n",
    "        _, _, _, _, _, os = self.forward_mbatch(x)\n",
    "        loss = self.loss_mbatch (os, y_onehot)\n",
    "        accuracy=self.accuracy_mbatch (os.argmax(axis=1), y)\n",
    "        return loss, accuracy\n",
    "    \n",
    "    \n",
    "    def finite_difference():\n",
    "        \n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment to get non-deterministic results\n",
    "np.random.seed(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 784), (10000, 1), (50000, 784), (50000, 1))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_valid.shape,y_valid.shape,x_train.shape,y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot(y, n_classes):\n",
    "    o = np.zeros(shape=(y.shape[0], n_classes))\n",
    "    for i in range(y.shape[0]):\n",
    "        o[i, int(y[i])] = 1\n",
    "    return o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation for 1 exemple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a1 shape =  (500,)\n",
      "h1 shape =  (500,)\n",
      "a2 shape =  (300,)\n",
      "h2 shape =  (300,)\n",
      "oa shape =  (10,)\n",
      "os shape =  (10,)\n"
     ]
    }
   ],
   "source": [
    "# self, input_dim, output_dim,hidden_dims=(1024,2048),n_hidden=2, initialization=zeros, mode=',train',\n",
    "# datapath=None,model_path=None\n",
    "x=x_train[0]\n",
    "y=y_train[0]\n",
    "y = np.zeros(shape=(10, ))\n",
    "y[y_train[0]] = 1\n",
    "\n",
    "NN_model= NN(784, 10, hidden_dims=(500,300),initialization='glorot')\n",
    "\n",
    "a1, h1, a2, h2, oa, os = NN_model.forward(x_train[0])\n",
    "\n",
    "#self,cache, x, y,a1, h1, a2, h2, oa, os, weight_decay=0)\n",
    "grads=NN_model.backward(x, y, a1, h1, a2, h2, oa, os)\n",
    "\n",
    "print ('a1 shape = ', a1.shape)\n",
    "print ('h1 shape = ', h1.shape)\n",
    "print ('a2 shape = ', a2.shape)\n",
    "print ('h2 shape = ', h2.shape)\n",
    "print ('oa shape = ', oa.shape)\n",
    "print ('os shape = ', os.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  0  loss  2.3025850929940455\n",
      "epoch  1  loss  2.2130472649209176\n",
      "epoch  2  loss  2.125400286246365\n",
      "epoch  3  loss  2.039751468982383\n",
      "epoch  4  loss  1.956204563387701\n",
      "epoch  5  loss  1.8748581566171187\n",
      "epoch  6  loss  1.7958040494881427\n",
      "epoch  7  loss  1.7191256666613888\n",
      "epoch  8  loss  1.644896559738838\n",
      "epoch  9  loss  1.5731790633976404\n"
     ]
    }
   ],
   "source": [
    "#training 10 epoch for 1 exemple\n",
    "\n",
    "#x, y_onehot, n, learning_rate=1e-1, weight_decay=0\n",
    "epochs=10\n",
    "for epoch in range (epochs):\n",
    "    loss=NN_model.train_SGD(x, y, 1)\n",
    "    print('epoch ', epoch, ' loss ', loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the best hyperparameters with validation dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions declaration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model using different hyperparameters, like mini-batch size, learning rate and epochs number\n",
    "# x_ds is the dataset to train, y_ds is the target dataset.\n",
    "def hyperparameter_checking(x_train_ds,y_train_ds,x_valid_ds,y_valid_ds,epochs,minibatch,learningrate,verbose=False):\n",
    "    x = x_train_ds\n",
    "    print('x.shape = ', x.shape)\n",
    "    y = y_train_ds\n",
    "    y=onehot(y,10)\n",
    "    print('y.shape = ', y.shape)\n",
    "    y_valid_onehot=onehot(y_valid_ds,10)\n",
    "    # input_dim, output_dim,hidden_dims,n_hidden=2,mode=',train',\n",
    "    # datapath=None,model_path=None\n",
    "    NN_mbatch_1= NN(784, 10, hidden_dims=(500,300),initialization='glorot')\n",
    "    loss_training_arr=[]\n",
    "    loss_validation_arr=[]\n",
    "    loss_mbatch_1=0\n",
    "    for epoch in range (epochs): \n",
    "        #x, y_onehot, mb_size=100, learning_rate=1e-1, weight_decay=0\n",
    "        loss_mbatch_1=NN_mbatch_1.train_mbatch(x, y, mb_size=minibatch,learning_rate=learningrate)\n",
    "        if verbose:\n",
    "            print('epoch ', epoch, ' loss ', loss_mbatch_1)\n",
    "        loss,accuracy=NN_mbatch_1.test_mbatch(x_valid_ds,y_valid_onehot,y_valid_ds)\n",
    "        loss_training_arr.append(loss_mbatch_1)\n",
    "        loss_validation_arr.append(loss)\n",
    "    print('epoch ', epoch, ' loss ', loss_mbatch_1)\n",
    "    return loss_training_arr,loss_validation_arr,accuracy,NN_mbatch_1 # accuracy of validation test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "# Display differents graphs using different hyperparameters saved in arrays.\n",
    "# Input variables are arrays with the same lenght, with the values to graph\n",
    "\n",
    "def show_graphs(minibatch_arr,epoch_arr,learningrate_arr,loss_arr):\n",
    "# define a list of markevery cases to plot\n",
    "    cases=[]\n",
    "    x_values=[]\n",
    "    y_values=[]\n",
    "    for i in range(len(minibatch_arr)):\n",
    "        cases.append([minibatch_arr[i],epoch_arr[i],learningrate_arr[i]])\n",
    "        # define the data for cartesian plots\n",
    "        x_values.append(range(epoch_arr[i]))\n",
    "        y_values.append(loss_arr[i])\n",
    "    # define the figure size and grid layout properties\n",
    "    figsize = (12, 10)\n",
    "    cols = 2\n",
    "    gs = gridspec.GridSpec(len(cases) // cols + 1, cols)\n",
    "    gs.update(hspace=0.4)\n",
    "    \n",
    "    fig1 = plt.figure(num=1, figsize=figsize)\n",
    "    ax = []\n",
    "    for i, case in enumerate(cases):\n",
    "        row = (i // cols)\n",
    "        col = i % cols\n",
    "        ax.append(fig1.add_subplot(gs[row, col]))\n",
    "        ax[-1].set_title('Minibatch=%s, Epoch=%s, Learning=%s' % (str(case[0]),str(case[1]),str(case[2])))\n",
    "        ax[-1].plot(x_values[i], y_values[i], 'o', ls='-', ms=4, markevery=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using different hyperparameters with validation set and Glorot initialization:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## * Changing Mini-batch size:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With different Mini-batch sizes, Epoch=100, Learning rate=0.1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ds=x_train\n",
    "y_ds=y_train\n",
    "x_valid_ds=x_valid\n",
    "y_valid_ds=y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape =  (50000, 784)\n",
      "y.shape =  (50000, 10)\n"
     ]
    }
   ],
   "source": [
    "loss_arr_0=[] #training loss array\n",
    "loss_arr_valid_0=[] #valid loss array\n",
    "epoch_arr_0=[] #epoch array (save the epoch used on each mini-batch)\n",
    "acc_arr_0=[] # accuracy with validation set array \n",
    "lr_0=1e-1 #learning rate used.\n",
    "epoch_0=50 #total epoch \n",
    "for i, minibatch in enumerate([50,100,150,500]):\n",
    "    loss_array,loss_array_valid,acc_valid,nn_model=hyperparameter_checking(x_ds,y_ds,x_valid_ds,y_valid_ds,epoch_0,minibatch,lr_0)\n",
    "    loss_arr_0.append(loss_array)\n",
    "    loss_arr_valid_0.append(loss_array_valid)\n",
    "    epoch_arr_0.append(epoch_0)\n",
    "    acc_arr_0.append(acc_valid)\n",
    "    print('Accuracy: %s' % str(acc_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_arr_0=[50,50,50,50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minibatch_arr=[50,100,150,500]\n",
    "fig, ax = plt.subplots(1,3,figsize=(18, 5))\n",
    "# plt.xlabel('Epoch')\n",
    "ax[2].plot(minibatch_arr, acc_arr_0, markevery=1)\n",
    "ax[2].set_xlabel('Mini-Batch')\n",
    "ax[2].set_ylabel('Accuracy')\n",
    "ax[2].set_title('Accuracy with different Mini-batch size')\n",
    "ax[2].grid(True)\n",
    "ax[0].plot(range(epoch_arr_0[3]), loss_arr_0[3], label='Mini-batch: %s' % str(minibatch_arr[3]))\n",
    "ax[0].plot(range(epoch_arr_0[2]), loss_arr_0[2], label='Mini-batch: %s' % str(minibatch_arr[2]))\n",
    "ax[0].plot(range(epoch_arr_0[1]), loss_arr_0[1], label='Mini-batch: %s' % str(minibatch_arr[1]))\n",
    "ax[0].plot(range(epoch_arr_0[0]), loss_arr_0[0], label='Mini-batch: %s' % str(minibatch_arr[0]))\n",
    "ax[0].legend()\n",
    "ax[0].set_xlabel('Epoch')\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[0].set_title('Total Loss per Epoch - Training Set')\n",
    "ax[1].plot(range(epoch_arr_0[3]), loss_arr_valid_0[3], label='Mini-batch: %s' % str(minibatch_arr[3]))\n",
    "ax[1].plot(range(epoch_arr_0[2]), loss_arr_valid_0[2], label='Mini-batch: %s' % str(minibatch_arr[2]))\n",
    "ax[1].plot(range(epoch_arr_0[1]), loss_arr_valid_0[1], label='Mini-batch: %s' % str(minibatch_arr[1]))\n",
    "ax[1].plot(range(epoch_arr_0[0]), loss_arr_valid_0[0], label='Mini-batch: %s' % str(minibatch_arr[0]))\n",
    "ax[1].legend()\n",
    "ax[1].set_xlabel('Epoch')\n",
    "ax[1].set_ylabel('Loss')\n",
    "ax[1].set_title('Total Loss per Epoch - Validation Set')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learningrate_arr=[lr_0,lr_0,lr_0,lr_0]\n",
    "show_graphs(minibatch_arr,epoch_arr_0,learningrate_arr,loss_arr_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We obtain a better accuracy when we choose a smaller Mini-Batch size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## * Changing Learning-Rate:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With different Learning Rates, Epoch=100, Minibath size=50:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ds=x_train\n",
    "y_ds=y_train\n",
    "x_valid_ds=x_valid\n",
    "y_valid_ds=y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_arr_1=[] #learning rate array.\n",
    "loss_arr_1=[]   #training loss array\n",
    "loss_arr_valid_1=[] #valid loss array\n",
    "epoch_arr_1=[] #epoch array (save the epoch used on each mini-batch)\n",
    "acc_arr_1=[] # accuracy with validation set array \n",
    "epoch_1=50  #total epoch \n",
    "mb=50\n",
    "for i, learningrate in enumerate([0.1,0.01,0.001]):\n",
    "    loss_array,loss_array_valid,acc_valid,nn_model=hyperparameter_checking(x_ds,y_ds,x_valid_ds,y_valid_ds,epoch_1,mb,learningrate)\n",
    "    loss_arr_1.append(loss_array)\n",
    "    loss_arr_valid_1.append(loss_array_valid)\n",
    "    epoch_arr_1.append(epoch_1)\n",
    "    acc_arr_1.append(acc_valid)\n",
    "    print('Accuracy: %s' % str(acc_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learningrate_arr=[0.1,0.01,0.001]\n",
    "fig, ax = plt.subplots(1,3,figsize=(18, 5))\n",
    "# plt.xlabel('Epoch')\n",
    "ax[2].plot(learningrate_arr, acc_arr_1, markevery=1)\n",
    "ax[2].set_xlabel('Learning Rate')\n",
    "ax[2].set_ylabel('Accuracy')\n",
    "ax[2].set_title('Accuracy with different Learning Rate')\n",
    "ax[2].grid(True)\n",
    "ax[0].plot(range(epoch_arr_1[2]), loss_arr_1[2], label='Learning Rate: %s' % str(learningrate_arr[2]))\n",
    "ax[0].plot(range(epoch_arr_1[1]), loss_arr_1[1], label='Learning Rate: %s' % str(learningrate_arr[1]))\n",
    "ax[0].plot(range(epoch_arr_1[0]), loss_arr_1[0], label='Learning Rate: %s' % str(learningrate_arr[0]))\n",
    "ax[0].legend()\n",
    "ax[0].set_xlabel('Epoch')\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[0].set_title('Total Loss per Epoch - Training Set')\n",
    "ax[1].plot(range(epoch_arr_1[2]), loss_arr_valid_1[2], label='Learning Rate: %s' % str(learningrate_arr[2]))\n",
    "ax[1].plot(range(epoch_arr_1[1]), loss_arr_valid_1[1], label='Learning Rate: %s' % str(learningrate_arr[1]))\n",
    "ax[1].plot(range(epoch_arr_1[0]), loss_arr_valid_1[0], label='Learning Rate: %s' % str(learningrate_arr[0]))\n",
    "ax[1].legend()\n",
    "ax[1].set_xlabel('Epoch')\n",
    "ax[1].set_ylabel('Loss')\n",
    "ax[1].set_title('Total Loss per Epoch - Validation Set')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "minibatch_arr=[50,50,50]\n",
    "show_graphs(minibatch_arr,epoch_arr_1,learningrate_arr,loss_arr_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We get a better accuracy if we choose a  lower learning rate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## * Changing Epochs:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With different Epochs, Learning Rates=0.1, Minibath size=50:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ds=x_train\n",
    "y_ds=y_train\n",
    "x_valid_ds=x_valid\n",
    "y_valid_ds=y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_arr_2=[] #learning rate array.\n",
    "loss_arr_2=[]   #training loss array\n",
    "loss_arr_valid_2=[] #valid loss array\n",
    "epoch_arr_2=[] #epoch array (save the epoch used on each mini-batch)\n",
    "acc_arr_2=[] # accuracy with validation set array \n",
    "learning_rate=0.1\n",
    "mb=50\n",
    "for i, epochs in enumerate([30,50,80,100]):\n",
    "    loss_array,loss_array_valid,acc_valid,nn_model=hyperparameter_checking(x_ds,y_ds,x_valid_ds,y_valid_ds,epochs,mb,learning_rate)\n",
    "    loss_arr_2.append(loss_array)\n",
    "    loss_arr_valid_2.append(loss_array_valid)\n",
    "    epoch_arr_2.append(epochs)\n",
    "    acc_arr_2.append(acc_valid)\n",
    "    print('Accuracy: %s' % str(acc_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_arr=[30,50,80,100]\n",
    "fig, ax = plt.subplots(1,3,figsize=(18, 5))\n",
    "# plt.xlabel('Epoch')\n",
    "ax[2].plot(epochs_arr, acc_arr_2, markevery=1)\n",
    "ax[2].set_xlabel('Epoch')\n",
    "ax[2].set_ylabel('Accuracy')\n",
    "ax[2].set_title('Accuracy with different Epochs')\n",
    "ax[2].grid(True)\n",
    "ax[0].plot(range(epoch_arr_2[3]), loss_arr_2[3], label='Epoch: %s' % str(epochs_arr[3]))\n",
    "ax[0].plot(range(epoch_arr_2[2]), loss_arr_2[2], label='Epoch: %s' % str(epochs_arr[2]))\n",
    "ax[0].plot(range(epoch_arr_2[1]), loss_arr_2[1], label='Epoch: %s' % str(epochs_arr[1]))\n",
    "ax[0].plot(range(epoch_arr_2[0]), loss_arr_2[0], label='Epoch: %s' % str(epochs_arr[0]))\n",
    "ax[0].legend()\n",
    "ax[0].set_xlabel('Epoch')\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[0].set_title('Total Loss per Epoch - Training Set')\n",
    "ax[1].plot(range(epoch_arr_2[3]), loss_arr_valid_2[3], label='Epoch: %s' % str(epochs_arr[3]))\n",
    "ax[1].plot(range(epoch_arr_2[2]), loss_arr_valid_2[2], label='Epoch: %s' % str(epochs_arr[2]))\n",
    "ax[1].plot(range(epoch_arr_2[1]), loss_arr_valid_2[1], label='Epoch: %s' % str(epochs_arr[1]))\n",
    "ax[1].plot(range(epoch_arr_2[0]), loss_arr_valid_2[0], label='Epoch: %s' % str(epochs_arr[0]))\n",
    "ax[1].legend()\n",
    "ax[1].set_xlabel('Epoch')\n",
    "ax[1].set_ylabel('Loss')\n",
    "ax[1].set_title('Total Loss per Epoch - Validation Set')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "minibatch_arr=[50,50,50,50]\n",
    "show_graphs(minibatch_arr,epoch_arr_1,learningrate_arr,loss_arr_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The best Accuracy was found with Epoch=80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The best hyperparameters to train the model are Mini-batch size: 50 and Learning Rate: 0.1 and Epoch:80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model with Mini-batch size=50, Epoch=80 and Learning rate=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_arr=[]\n",
    "epochs=80\n",
    "mb=50\n",
    "lr=1e-1\n",
    "x=x_train\n",
    "y=onehot(y_train,10)\n",
    "# input_dim, output_dim,hidden_dims,n_hidden=2,mode=',train',\n",
    "# datapath=None,model_path=None\n",
    "NN_mbatch_1= NN(784, 10, hidden_dims=(500,300),initialization='glorot')\n",
    "for epoch in range (epochs): \n",
    "    #x, y_onehot, mb_size=100, learning_rate=1e-1, weight_decay=0\n",
    "    loss_mbatch_1=NN_mbatch_1.train_mbatch(x, y, mb_size=50,learning_rate=0.1)\n",
    "    print('epoch ', epoch, ' loss ', loss_mbatch_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The accuracy obtained by the model using the training set is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=NN_mbatch_1.prediction_mbatch(x_train)\n",
    "accuracy=NN_mbatch_1.accuracy_mbatch(y_pred,y_train)\n",
    "print('Accuracy: %s' % str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The accuracy obtained by the model using the testing set is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test=NN_mbatch_1.prediction_mbatch(X_test)\n",
    "accuracy=NN_mbatch_1.accuracy_mbatch(y_pred_test,Y_test)\n",
    "print('Accuracy: %s' % str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================================================================="
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
